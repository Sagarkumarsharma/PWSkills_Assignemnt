# -*- coding: utf-8 -*-
"""Predict the optimum numbers of cluster.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gKH5K23fyr5dg417AapqK6hmFgsDCSCJ

**Name: Sagar Kumar Sharma**

***Spark Foundation Task 2: Predict the optimum number of clusters and represent it visually.***

Dataset : https://bit.ly/3kXTdox

**Clustering :** The technique to segregate Datasets into various groups, on basis of having similar features and characteristics, is being called Clustering.

**K-Means :**Kmeans Algorithm is an Iterative algorithm that divides a group of n datasets into k subgroups /clusters based on the similarity and their mean distance from the centroid of that particular subgroup/ formed.
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import datasets

"""**Load Dataset**"""

#load the iris dataset
iris = datasets.load_iris()
iris_df = pd.DataFrame(iris.data, columns = iris.feature_names)
iris_df.head()  #first 5 rows

"""**Finding the Optimum number of clusters for K-Means and determining the value of K**"""

#finding the optimum number of clusters for k-means classfication
x = iris_df.iloc[:, [0,1,2,3]].values

from sklearn.cluster import KMeans
wcss = []

for i in range(1, 11):
  kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)

  kmeans.fit(x)
  wcss.append(kmeans.inertia_)

#plotting the results onto a line graph
#allowing us to observe "the elbow"
plt.plot(range(1, 11), wcss)
plt.title("the elbow method")
plt.xlabel("number of clusters")
plt.ylabel("WCSS")  #within clusters sum of squares
plt.show()

"""You can clearly see why it is called 'The elbow method' from the above graph, the optimum clusters is where the elbow occurs. This is when the within cluster sum of squares (WCSS) doesn't decrease significantly with every iteration.

From this we choose the number of clusters as 3.
"""

#applying kmeans to the dataset/creating kmeans classfier
kmeans = KMeans(n_clusters = 3, init = "k-means++",
                max_iter = 300, n_init = 10, random_state = 0)
y_kmeans = kmeans.fit_predict(x)

"""**Visualising the clusters**"""

#on the first two columns
plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1],
            s = 100, c = "Orange", label = "Iris-Setosa"
            )
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1],
            s = 100, c = "green", label = "Iris-Setosa"
            )
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1],
            s = 100, c = "blue", label = "Iris-Setosa"
            )
#plotting the cetroids of the clusters
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1],
            s = 100, c = "yellow", label = "Centroids")

plt.legend()